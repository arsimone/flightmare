policy: 'CustomActorCriticPolicy'

tcn_queue_size: 40    # 80*0.02 = 1.6 seconds of data 
tcn_history_size: 8   # 0.02*(80/16) = 0.02*5 = 0.1 seconds betweem each sample
tcn_net_arch: [512, 512, 512, 512]

net_arch_pi: [256, 256, 128]
net_arch_vf: [256, 256, 128]
reward_baseline: 1000
n_steps: 10000
gae_lambda: 0.95
gamma: 0.99
ent_coef: 0.0
learning_rate_const: 0.0003
schedule_lr: True
schedule_lr_start_value: 0.0001
schedule_lr_end_value: 0.00001
vf_coef: 0.5
max_grad_norm: 0.5
n_epochs: 10
clip_range: 0.2
n_iterations: 5000 
# num_steps = n_iterations * num_envs * n_steps
checkpoint_interval: 3000000 